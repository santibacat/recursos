{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF 3 NLP.ipynb","provenance":[{"file_id":"https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%201%20-%20Exercise-answer.ipynb","timestamp":1578433025744}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["CURSO TENSORFLOW 3\n","SEMANA 1\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","el tokenizer convierte palabras de frases en un diccionario y mapea cada palabra a un id\n","\n","le tenemos que pasar:\n","- un maximo de palabras (num_words =100 p.ej) que es el maximo de palabras que se guardan (si hay más simplemente tiene en cuenta las más frecuentes).\n","- si queremos guardar los caracteres que no estan en el diccionario (palabras raras por ej), debemos añadir oov_token = ‘<OOV>’, significa out of vocabulary\n","\n","luego le pasamos el tokenizador a nuestro texto para que lo convierta en el diccionario:\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","El tokenizador no tiene en cuenta ni las exclamaciones, ni las mayusculas ni la puntuacion.\n","Ej: dog y Dog y Dog! son lo mismo\n","\n","Ahora para convertirlo en una frase vectorizada haremos:\n","sequences (que no sentences = tokenizer.texts_to_sequences(sentences)\n","\n","Para convertir frases en padding debemos cargar\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","padded = pad_sequences(sequences)\n","Esto hace padding hasta como maximo la frase más larga (añade ceros antes).\n","Si queremos padding despues y con un máximo:\n","\t(padding =‘post’, maxlen=10)\n"]},{"cell_type":"code","metadata":{"id":"zrZevCPJ92HG","colab_type":"code","colab":{}},"source":["# Paquetes necesarios\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","\n","#Stopwords list\n","stopwords = [ \"a\", \"about\", \"above\"]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rmYBjsyCv3K","colab_type":"code","colab":{}},"source":["# Importacion de CSV\n","sentences = []\n","labels = []\n","with open(\"/tmp/bbc-text.csv\", 'r') as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    next(reader) # sirve para no leer la primera línea\n","    for row in reader:\n","        labels.append(row[0])\n","        sentence = row[1]\n","        for word in stopwords:\n","            token = \" \" + word + \" \"\n","            sentence = sentence.replace(token, \" \")\n","            sentence = sentence.replace(\"  \", \" \")\n","        sentences.append(sentence)\n","\n","# Importación de JSON\n","import json\n","with open('/../test.json', 'r') as f:\n","    datastore = json.load(f)\n","  \n","  for item in datastore:\n","    # append if necessary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LhzBBgSC3S5","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","print(len(word_index))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Gr3dbQfC5VR","colab_type":"code","colab":{}},"source":["sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, padding='post')\n","\n","print(padded[0])\n","print(padded.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZufOahzC6yx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}