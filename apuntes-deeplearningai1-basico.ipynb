{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Apuntes curso 1 tf DL.ai.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bLVfFODNcWGF","colab_type":"text"},"source":["Semana 2 (modelo mnist básico, sin CNN)"]},{"cell_type":"code","metadata":{"id":"0lE4tn6gccVJ","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","class myCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs={}):\n","    if(logs.get('acc')>0.99):\n","      print(\"\\nReached 99% accuracy so cancelling training!\")\n","      self.model.stop_training = True\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","callbacks = myCallback()\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n","  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qSDbAvc5ciIH","colab_type":"text"},"source":["Semana 3 (mnist con CNN)"]},{"cell_type":"code","metadata":{"id":"Ra1o866Fcl9s","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","print(tf.__version__)\n","\n","class myCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs={}):\n","    if(logs.get('acc')>0.998):\n","      print(\"\\nReached 99.8% accuracy so cancelling training!\")\n","      self.model.stop_training = True\n","\n","callbacks = myCallback()\n","\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\n","test_loss = model.evaluate(test_images, test_labels)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdM-r-8HdHb6","colab_type":"text"},"source":["Semana 4 (fashion mnist con CNN)"]},{"cell_type":"code","metadata":{"id":"EwAfu-msdLUJ","colab_type":"code","colab":{}},"source":["import os\n","import zipfile\n","\n","local_zip = '/tmp/horse-or-human.zip' # establece el archivo zip\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/horse-or-human')\n","zip_ref.close() # lo extrae y lo cierra\n","\n","# Establecer directorio de entrenamiento\n","train_dir = os.path.join('/tmp/horse-or-human/horses')\n","# Mostrar los nombres y numero total de los archivos\n","train_names = os.listdir(train_dir)\n","print(train_names[:10])\n","print('total training  images:', len(os.listdir(train_dir)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jBfEkX9h2SM","colab_type":"code","colab":{}},"source":["# Mostrar una imagen 4x4 de ejemplo\n","%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","# Parameters for our graph; we'll output images in a 4x4 configuration\n","nrows = 4\n","ncols = 4\n","\n","# Index for iterating over images\n","pic_index = 0\n","\n","# Set up matplotlib fig, and size it to fit 4x4 pics\n","fig = plt.gcf()\n","fig.set_size_inches(ncols * 4, nrows * 4)\n","\n","pic_index += 8\n","next_horse_pix = [os.path.join(train_horse_dir, fname) \n","                for fname in train_horse_names[pic_index-8:pic_index]]\n","next_human_pix = [os.path.join(train_human_dir, fname) \n","                for fname in train_human_names[pic_index-8:pic_index]]\n","\n","for i, img_path in enumerate(next_horse_pix+next_human_pix):\n","  # Set up subplot; subplot indices start at 1\n","  sp = plt.subplot(nrows, ncols, i + 1)\n","  sp.axis('Off') # Don't show axes (or gridlines)\n","\n","  img = mpimg.imread(img_path)\n","  plt.imshow(img)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXrVq3mrh_rs","colab_type":"code","colab":{}},"source":["# Crear el modelo\n","model = tf.keras.models.Sequential([\n","    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n","    # This is the first convolution\n","    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    ### más capas en medio\n","    tf.keras.layers.Flatten(),\n","    # 512 neuron hidden layer\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.summary()\n","\n","from tensorflow.keras.optimizers import RMSprop\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(lr=0.001),\n","              metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfbuILLFiPm6","colab_type":"code","colab":{}},"source":["# Preprocesamiento: data generators\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","train_generator = train_datagen.flow_from_directory(\n","        '/tmp/horse-or-human/',  # Carpeta RAIZ, no subcarpetas\n","        target_size=(300, 300),  # Tamaño de reescalado\n","        batch_size=128,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","# Entrenamiento\n","history = model.fit_generator(\n","      train_generator,\n","    #validation_data=validation_generator\n","    #validation_steps=50\n","      steps_per_epoch=8,  \n","      epochs=15,\n","      verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"unIfi3BEtgck","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc      = history.history[     'acc' ]\n","val_acc  = history.history[ 'val_acc' ]\n","loss     = history.history[    'loss' ]\n","val_loss = history.history['val_loss' ]\n","\n","epochs   = range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot  ( epochs,     acc )\n","plt.plot  ( epochs, val_acc )\n","plt.title ('Training and validation accuracy')\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot  ( epochs,     loss )\n","plt.plot  ( epochs, val_loss )\n","plt.title ('Training and validation loss'   )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z9JZaeWSifSn","colab_type":"code","colab":{}},"source":["# Ejecutar el modelo SUBIENDO un archivo\n","import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(300, 300))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a human\")\n","  else:\n","    print(fn + \" is a horse\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_L5fSSw1tb1x","colab_type":"code","colab":{}},"source":["# Representaciones intermedias\n","import numpy as np\n","import random\n","from   tensorflow.keras.preprocessing.image import img_to_array, load_img\n","\n","# Let's define a new Model that will take an image as input, and will output\n","# intermediate representations for all layers in the previous model after\n","# the first.\n","successive_outputs = [layer.output for layer in model.layers[1:]]\n","\n","#visualization_model = Model(img_input, successive_outputs)\n","visualization_model = tf.keras.models.Model(inputs = model.input, \n","                                            outputs = successive_outputs)\n","\n","# Let's prepare a random input image of a cat or dog from the training set.\n","cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\n","dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n","\n","img_path = random.choice(cat_img_files + dog_img_files)\n","img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n","\n","x   = img_to_array(img)                # Numpy array with shape (150, 150, 3)\n","x   = x.reshape((1,) + x.shape)       # Numpy array with shape (1, 150, 150, 3)\n","\n","# Rescale by 1/255\n","x /= 255.0\n","\n","# Let's run our image through our network, thus obtaining all\n","# intermediate representations for this image.\n","successive_feature_maps = visualization_model.predict(x)\n","\n","# These are the names of the layers, so can have them as part of our plot\n","layer_names = [layer.name for layer in model.layers]\n","\n","# -----------------------------------------------------------------------\n","# Now let's display our representations\n","# -----------------------------------------------------------------------\n","for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n","  \n","  if len(feature_map.shape) == 4:\n","    \n","#-------------------------------------------\n","# Just do this for the conv / maxpool layers, not the fully-connected layers\n","#-------------------------------------------\n","n_features = feature_map.shape[-1]  # number of features in the feature map\n","size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n","\n","# We will tile our images in this matrix\n","display_grid = np.zeros((size, size * n_features))\n","    \n","#-------------------------------------------------\n","# Postprocess the feature to be visually palatable\n","#-------------------------------------------------\n","for i in range(n_features):\n","  x  = feature_map[0, :, :, i]\n","  x -= x.mean()\n","  x /= x.std ()\n","  x *=  64\n","  x += 128\n","  x  = np.clip(x, 0, 255).astype('uint8')\n","  display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n","\n","#-----------------\n","# Display the grid\n","#-----------------\n","scale = 20. / n_features\n","plt.figure( figsize=(scale * n_features, scale) )\n","plt.title ( layer_name )\n","plt.grid  ( False )\n","plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvADI7iuik86","colab_type":"code","colab":{}},"source":["#limpiar antes del siguiente ejercicio\n","import os, signal\n","os.kill(os.getpid(), signal.SIGKILL)"],"execution_count":0,"outputs":[]}]}